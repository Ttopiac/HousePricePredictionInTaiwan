{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A top 1% Model for 2019 Esun AI Competition: Taiwan House Price Prediction\n",
    "## This is an ensemble learning model, composed with a base model and a model, modified based on the city feature\n",
    "## In this file, the modified model will be explained briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "from scipy.special import boxcox1p\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm, skew #for some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions, which will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers\n",
    "# outliers: house with incredible huge building area\n",
    "def DropIndexs(dataset):\n",
    "    all_data = dataset.copy()\n",
    "    all_data = all_data.drop(all_data[(all_data['building_area']>80)].index)\n",
    "    return all_data\n",
    "\n",
    "# Log-normal transformation on the hosue price feature\n",
    "def PreProcessingForOnlyTr(dataset):\n",
    "    all_data = dataset.copy()\n",
    "    all_data[\"total_price\"]=all_data['total_price']/all_data['building_area']\n",
    "    all_data[\"total_price\"] = np.log1p(all_data[\"total_price\"])\n",
    "    return all_data  \n",
    "\n",
    "# data preprocessing, \n",
    "# including creating hidden features, filling missing values, \n",
    "# dropping bad features and some other feature engineering.\n",
    "def PreProcessing(dataset):\n",
    "    all_data = dataset.copy()\n",
    "    ## Variable Transformation\n",
    "    ## Predict price per sq\n",
    "    ## Log Trasnformation\n",
    "    all_data[\"building_area\"] = np.log1p(all_data[\"building_area\"])\n",
    "    all_data[\"land_area\"] = np.log1p(all_data[\"land_area\"])\n",
    "    ### Feature Engineering\n",
    "    all_data['N_500']=all_data['N_500']-all_data['N_50']\n",
    "    all_data['N_1000']=all_data['N_1000']-all_data['N_500']-all_data['N_50']\n",
    "    all_data['N_5000']=all_data['N_5000']-all_data['N_1000']-all_data['N_500']-all_data['N_50']\n",
    "    all_data['N_10000']=all_data['N_10000']-all_data['N_5000']-all_data['N_1000']-all_data['N_500']-all_data['N_50']\n",
    "    # # Filling missing values\n",
    "    all_data['txn_floor']=all_data['txn_floor'].fillna(0)\n",
    "    all_data['parking_price']=all_data['parking_price'].fillna(0)\n",
    "    all_data['parking_area']=all_data['parking_area'].fillna(0)\n",
    "    all_data[\"parking_area\"] = np.log1p(all_data[\"parking_area\"])\n",
    "    all_data['village_income_median']=all_data['village_income_median'].fillna(0)\n",
    "    # # Create hidden features\n",
    "    all_data['roof']=np.where(all_data['total_floor']==all_data['txn_floor'], 0, 1)\n",
    "    all_data['house_age']=all_data['txn_dt']-all_data['building_complete_dt']\n",
    "    all_data['material_price']=pd.np.where((all_data['building_material']==10)|(all_data['building_material']==9)|(all_data['building_material']==5), 0, 1)\n",
    "    all_data['building_material'] = all_data['building_material'].apply(str)\n",
    "    all_data['building_type'] = all_data['building_use'].apply(str)\n",
    "    all_data['building_use'] = all_data['building_type'].apply(str)\n",
    "    all_data['town'] = all_data['town'].apply(str)\n",
    "    all_data['year']=(all_data['building_complete_dt']/365).apply(str)\n",
    "    all_data['parking_way'] = all_data['parking_way'].apply(str)\n",
    "    all_data['txn_floor'] = all_data['txn_floor'].apply(str)\n",
    "    all_data['village']=all_data['village'].apply(str)\n",
    "    all_data['lon']=all_data['lon'].apply(str)\n",
    "    ########################################## Transform skewness\n",
    "    numeric_feats = all_data.dtypes[(all_data.dtypes != \"object\" ) & (all_data.columns != \"city\")].index\n",
    "    # # Check the skew of all numerical features\n",
    "    skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "    skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "    skewness = skewness.head(10)\n",
    "    skewness = skewness[abs(skewness) > 0.55]\n",
    "    skewed_features = skewness.index\n",
    "    lam = 0.6\n",
    "    for feat in skewed_features:\n",
    "        #all_data[feat] += 1\n",
    "        all_data[feat] = boxcox1p(all_data[feat], lam)\n",
    "    all_data=pd.get_dummies(all_data, columns=['txn_floor','lon','roof','year','town','building_material','building_use','building_type','village','material_price','parking_way'])\n",
    "    # ## drop Variable\n",
    "    all_data.drop(\"lat\", axis = 1, inplace = True)\n",
    "    all_data.drop(\"born_rate\", axis = 1, inplace = True)\n",
    "    all_data.drop(\"death_rate\", axis = 1, inplace = True)\n",
    "    all_data.drop(\"marriage_rate\", axis = 1, inplace = True)\n",
    "    all_data.drop(\"divorce_rate\", axis = 1, inplace = True)\n",
    "    return all_data\n",
    "\n",
    "# Post Processing\n",
    "def PostProcessingForThePrice(dataset, priceList):\n",
    "    priceList = np.expm1(priceList)\n",
    "    priceList = priceList*dataset['building_area'].values\n",
    "    # brutal force especially on high price data\n",
    "    q2 = np.quantile(priceList, .996)\n",
    "    priceList = np.where(priceList < q2, priceList, priceList * 1.2 )\n",
    "    return priceList\n",
    "\n",
    "# Align a dataset with another dataset\n",
    "# After this process,  oridata will be added columns(features), \n",
    "# which was not existed in it, but existed in the target data.  \n",
    "def GetMissingColumns( oriData, target):\n",
    "    # Get missing columns in the training test\n",
    "    missing_cols = set( target.columns ) - set( oriData.columns )\n",
    "    # Add a missing column in test set with default value equal to 0\n",
    "    for c in missing_cols:\n",
    "        if c is not \"total_price\":\n",
    "            oriData[c] = 0\n",
    "    # Ensure the order of column in the test set is in the same order than in train set\n",
    "    oriData = oriData[target.columns]\n",
    "    return oriData\n",
    "\n",
    "# Get data of the city\n",
    "def get_dataset_of_the_city(tr, the_city):\n",
    "    trOfTheCity = tr.copy()\n",
    "    city_name = str(the_city)\n",
    "    #Extracy the data only which is related to the city\n",
    "    trOfTheCity = trOfTheCity.drop(trOfTheCity[(trOfTheCity['city']!=the_city)].index)\n",
    "    return trOfTheCity\n",
    "\n",
    "# Drop data of the city\n",
    "def get_dataset_except_the_city(tr, the_city):\n",
    "    trOfTheCity = tr.copy()\n",
    "    city_name = str(the_city)\n",
    "    #Extracy the data only which is related to the city\n",
    "    trOfTheCity = trOfTheCity.drop(trOfTheCity[(trOfTheCity['city']==the_city)].index)\n",
    "    return trOfTheCity\n",
    "\n",
    "# According to different city feature, this function extracted different trainset. \n",
    "# Each trainset drops a part of data based on an experiment.\n",
    "# \n",
    "# In that experiment, 11 types of dataset were trained to get 11 trained models.(Each type of dataset drops data of a city.)\n",
    "# Then, all 11 models are used to predict the house price of data of 11 cities. \n",
    "# Hit rate of all 121 results are measured.\n",
    "# In the end, the best trained model for prediction of data of each city will be found.   \n",
    "def get_dataset_for_the_city(tr, the_city):\n",
    "    if the_city == 3:\n",
    "        return get_dataset_except_the_city(tr, 17)\n",
    "    elif the_city == 5:    \n",
    "        return get_dataset_except_the_city(tr, 7)\n",
    "    elif the_city == 6:\n",
    "        return get_dataset_except_the_city(tr, 12)\n",
    "    elif the_city == 7:\n",
    "        return get_dataset_except_the_city(tr, 12)\n",
    "    elif the_city == 10:\n",
    "        return get_dataset_except_the_city(tr, 12)\n",
    "    elif the_city == 12:\n",
    "        return get_dataset_except_the_city(tr, 5)\n",
    "    elif the_city == 21:\n",
    "        return get_dataset_except_the_city(tr, 9)\n",
    "    elif the_city == 9:\n",
    "        return get_dataset_except_the_city(tr, 21)\n",
    "    elif the_city == 13:\n",
    "        return get_dataset_except_the_city(tr, 21)\n",
    "    elif the_city == 14:\n",
    "        return get_dataset_except_the_city(tr, 21)\n",
    "    elif the_city == 17:\n",
    "        return get_dataset_except_the_city(tr, 5)\n",
    "    else:\n",
    "        return tr\n",
    "\n",
    "# Train the models for data of each city from learning individual suitable trainset.\n",
    "def Models_TheBestCities(city_list, trainset, models):\n",
    "    trainDict = {}\n",
    "    for i in city_list:\n",
    "        trainDict[str(i)] = get_dataset_for_the_city(trainset, i)\n",
    "        print(\"-****Model training for City \"+str(i)+\"*****-\")\n",
    "        trainOfCity = trainDict[str(i)]\n",
    "        # get the labels\n",
    "        y = trainOfCity.total_price.values\n",
    "        trainOfCity.drop(['building_id', 'total_price'], inplace=True, axis=1)\n",
    "        x = trainOfCity.values\n",
    "        # #\n",
    "        # # Create the LightGBM data containers\n",
    "        # #\n",
    "        model = lightgbm.LGBMRegressor(objective='regression',num_leaves=128,\n",
    "                                  learning_rate=0.012, n_estimators=21002,\n",
    "                                  max_bin = 1550, bagging_fraction = 0.8,\n",
    "                                  bagging_freq = 5, feature_fraction = 0.3319,\n",
    "                                  feature_fraction_seed=9, bagging_seed=9,\n",
    "                                  min_data_in_leaf =10, min_sum_hessian_in_leaf = 16)\n",
    "        model.fit(x,y)\n",
    "        models[str(i)] = model\n",
    "    return models\n",
    "\n",
    "# Get the prediction house price of testset by the trained models, specially designed for data of each city.\n",
    "def Get_predict_of_CityModels(cityList, originTest, afPreprocTest, modelDictionary, modelStr):\n",
    "    outputDict = {}\n",
    "    for i in cityList:\n",
    "        model = modelDictionary[modelStr+str(i)]\n",
    "        submissionByCity = get_dataset_of_the_city(afPreprocTest, i)\n",
    "        idsByCity = submissionByCity['building_id'].values\n",
    "        submissionByCity.drop(['building_id', 'total_price'], inplace=True, axis=1)\n",
    "        x = submissionByCity.values        \n",
    "        y = model.predict(x) \n",
    "        submissionBefPreprocByCity = get_dataset_of_the_city(originTest, i)\n",
    "        preds = PostProcessingForThePrice(submissionBefPreprocByCity, y)\n",
    "        outputDict[str(i)] = pd.DataFrame({'building_id': idsByCity, 'total_price': preds})\n",
    "    predictByModels = []\n",
    "    for i in afPreprocTest.index:\n",
    "        city = afPreprocTest['city'][i]\n",
    "        b_id = afPreprocTest['building_id'][i]\n",
    "        op = outputDict[str(city)]\n",
    "        the_price = op['total_price'].where(op['building_id'] == b_id)\n",
    "        the_price.dropna(inplace = True)  \n",
    "        predictByModels.extend(the_price)\n",
    "    return predictByModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "#### 1. read the file\n",
    "#### 2. drop the outliers\n",
    "#### 3. data preprocessing, including creating hidden features, filling missing values, dropping bad features and some other feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_data size is : (10000, 4211)\n",
      "train_data size is : (47820, 4211)\n"
     ]
    }
   ],
   "source": [
    "train_csv = pd.read_csv('train.csv')\n",
    "trainBefPreproc, valBefPreproc = train_test_split(train_csv, test_size=0.2, random_state=42)\n",
    "\n",
    "trainAfDrop = DropIndexs(trainBefPreproc)\n",
    "train = PreProcessingForOnlyTr(trainAfDrop)\n",
    "train = PreProcessing(train)\n",
    "    \n",
    "valAfDrop = DropIndexs(valBefPreproc)\n",
    "val = PreProcessingForOnlyTr(valAfDrop)\n",
    "val = PreProcessing(val)\n",
    "\n",
    "submissionBefPreproc = pd.read_csv('test.csv')\n",
    "submission = PreProcessing(submissionBefPreproc)\n",
    "\n",
    "if(len(train.columns)>=len(submission.columns)):\n",
    "    submission = GetMissingColumns(submission, train)\n",
    "    print(\"submission_data size is : {}\".format(submission.shape))    \n",
    "    train = GetMissingColumns(train, submission)\n",
    "    print(\"train_data size is : {}\".format(train.shape))\n",
    "else:\n",
    "    train = GetMissingColumns(train, submission)\n",
    "    print(\"train_data size is : {}\".format(train.shape))\n",
    "    submission = GetMissingColumns(submission, train)\n",
    "    print(\"submission_data size is : {}\".format(submission.shape)) \n",
    "    \n",
    "origin_tr = train.copy()\n",
    "origin_val = val.copy()\n",
    "global_city_list = [3, 5, 6, 7, 10, 12, 21, 9, 13, 14, 17] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-****Model training for City 3*****-\n",
      "-****Model training for City 5*****-\n",
      "-****Model training for City 6*****-\n",
      "-****Model training for City 7*****-\n",
      "-****Model training for City 10*****-\n",
      "-****Model training for City 12*****-\n",
      "-****Model training for City 21*****-\n",
      "-****Model training for City 9*****-\n",
      "-****Model training for City 13*****-\n",
      "-****Model training for City 14*****-\n",
      "-****Model training for City 17*****-\n"
     ]
    }
   ],
   "source": [
    "modelDict = {}\n",
    "modelDict = Models_TheBestCities(global_city_list, origin_tr, modelDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the hosuing price accodding to the testset by the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictByTheCityModel = Get_predict_of_CityModels(global_city_list, submissionBefPreproc, submission, modelDict, \"\")\n",
    "ids = submission['building_id'].values\n",
    "output = pd.DataFrame({'building_id': ids, 'total_price': predictByTheCityModel})\n",
    "output.to_csv(\"submissionByTheCityModel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
